---
title: "Churn Project"
author: "Sandra Martín-Forero Cogolludo"
date: "2/9/2024"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Churn Rate Project

## 1. Context and objetive of the project

Churn rate refers to the rate of customer's attrition, which means how many customers stop using a company's service within a certain period of time. The aim of this project is to understand better what characteristics or behaviours are related to the customers when they leave company's services.

Having understood this, the company is able to take into consideration new measures to improve customer retention, such as enhacing its services or personalizing offers.

## 2. Tools used

To develop this analysis, it has been used several different tools and libraries:

```{r librerías}
library(corrplot)
library(ggplot2)
library(reshape2)
library(tidyr)
library(dplyr)
library(DataExplorer)
library(caret)
library(randomForest)
library(ROCR)
```

*· Readxl:* It allows to read any database from Excel. In this project, it is used to read the data from a file .xlsx

*· Corrplot:* This library is used to create correlation plots. It helps to visualize how are the variable's interactions.

*· Ggplot2 and Reshape2:* These are libraries for data visualization. ggplot2 is used to create advanced plots, while reshape2 helps reorganize data to make it easier to plot.

*· Tidyr and Dplyr:* Two powerful tools that helps so much in terms of organization and manipulation.

*· DataExplorer:* A tool that facilitates the initial exploration of the data to understand its structure and characteristics.

*· Caret:* A library of predictive modeling tools. It is used here to prepare the data and build models that can predict customer churn.

*· RandomForest:* An advanced analysis method that creates multiple "decision trees" and uses them to make predictions. It is useful for understanding complex patterns in the data.

*· ROCR:* It is used to evaluate the accuracy of predictive models, helping to determine how well the model is predicting the churn rate.

## 3. Analysis steps

*Step 1: Data Loading*

The project begins by loading customer data from an Excel file (churn_all.xlsx). This data contains various pieces of information about each customer, such as:

· State of residence (e.g., KS, OH, NJ).
· Phone number.
· Whether they have an international plan or not.
· Whether they use voicemail.
· Call duration at different times of the day (day, night, evening).
· Number of times they called customer service.
· Whether or not they churned (left the service).

```{r Data Loading}
library(readxl)
churn_all <- read_excel("G:/Datos Sandra/Descargas/churn_all.xlsx")
View(churn_all)
```

*Step 2: Correlation Analysis*

A correlation matrix is created to identify the most important variables that are tightly linked to the churn rate.

*What is a correlation matrix?* Imagine a table that compares all the customer characteristics against each other. Each cell in the table shows how strongly two characteristics are related. A high value may indicate that two characteristics tend to increase or decrease together. For example, if the number of daytime call minutes and the total charges for daytime calls are highly correlated, it means when one increases, the other does as well.

*Why is it important?* It helps us identify which characteristics might be influencing a customer's decision to leave the company. For instance, if we find a high correlation between the number of calls to customer service and churn, we might infer that unresolved issues could be pushing customers to leave.

```{r correlation matrix}
numeric_cols <- churn_all[, c("state", "area_code", "phone_number", "international_plan", "voice_mail_plan", "number_vmail_messages", "total_day_minutes", "total_day_calls", "total_day_charge", "total_eve_minutes", "total_eve_calls", "total_eve_charge", "total_night_minutes", "total_night_calls", "total_night_charge", "total_intl_minutes", "total_intl_calls", "total_intl_charge", "number_customer_service_calls", "Churn")]

non_numeric <- churn_all[, sapply(numeric_cols, function(x) any(!is.numeric(x)))]
print(non_numeric)  #Since we already know that we have variables that are not numerical, we are going to develop correlation matrix taking into consideration numeric ones. 

numeric_cols <- churn_all[, c("number_vmail_messages", "total_day_minutes", "total_day_calls", "total_day_charge", "total_eve_minutes", "total_eve_calls", "total_eve_charge", "total_night_minutes", "total_night_calls", "total_night_charge", "total_intl_minutes", "total_intl_calls", "total_intl_charge", "number_customer_service_calls", "Churn")]
correlation_matrix <- cor(numeric_cols)

correlation_df <- melt(correlation_matrix)
ggplot(correlation_df, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "orange", mid = "white", midpoint = 0, 
limit = c(-1,1), space = "Lab", name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
coord_fixed()
```
The most important variables of the study are 'Total day minutes', 'Total day charge', 'Number customer service calls' y 'Churn', as we could see in the plot.

*Step 3: Classification of Customers by Usage Levels*

A new variable is created that classifies customers into two groups: High and Low usage, based on the total number of daytime call minutes.

*How is it defined?* If a customer has more than 180 minutes of daytime calls, they are classified as "High." If they have fewer, they are classified as "Low."

*Why is this done?* This classification helps segment customers and better understand which type of user is more likely to churn. For example, if we find that high-usage customers have a higher churn rate, the company might consider offering special plans or incentives to retain this group.

```{r classification of customers by usage levels}
umbral <- 180
consumption_level_grouping <- ifelse(churn_all$total_day_minutes >= umbral, "High", "Low")

print(consumption_level_grouping)
```

*Step 4: Data Visualization*

Charts are created to visualize the results of the correlation analysis.

*What is visualized?* The charts could show which characteristics are more related to churn. Using colors, sizes, and shapes, it becomes clear which relationships are stronger.

*Why is it useful?* Data visualization allows to quickly understand the results of the analysis without the need to interpret complex numbers.

```{r customer service calls plot}
ggplot(churn_all, aes(x = factor(Churn), y = number_customer_service_calls)) +
  geom_boxplot() +
  labs(x = "Churn", y = "Customer Service Calls") +
  theme_minimal()
```
This plot shows that as calls to technical service increase, the churn rate grows.These two variables are going to be especially significant in the study. 

*Step 5: Statistical analysis*

Now let's delve deeper into the statistical analysis of the data:

```{r str analysis}
head(churn_all)
str(churn_all)
summary(churn_all)
```
All the variables are transformed into factor to avoid future codyfing problems.

```{r transformation into factor}

churn_table <- mutate_if(churn_all, is.character, as.factor) 

churn_table <- mutate_if(churn_all, is.double, as.factor) 

churn_table <- mutate_if(churn_all, is.logical, as.factor) 

str(churn_table) 
```

```{r summary of variables and lost values}
plot_intro(churn_table)
```
Here we have a brief that shows that the database hasn't got any missing value. If the database had contained a missing value, it'd have been necessary to prepare a more extensive analysis, including NA's substitution.


```{r plot frequencies}
churn_table %>%
     gather() %>%
     ggplot(aes(value, fill = key)) +
     geom_bar() +
     facet_wrap(~ key, scales = "free") +
     theme(
         axis.text = element_text(size = 5),
         axis.title = element_text(size = 6),
         legend.text = element_text(size = 6),
         legend.position = "bottom")
```

*· Classification model construction*

In this point of the study, it is important to consider a method that includes only the variables that are significantly related to the churn. As we previously saw in 'step 2', these variables are 'Total day minutes', 'Total day charge', 'Number customer service calls'. It is better to discard the rest of the variables if we want to achieve a more accurate result.

```{r construction of the classification model with glm}
set.seed(123)  #For more reproducibility 
trainIndex <- createDataPartition(churn_table$Churn, p = 0.7, list = FALSE)
trainData <- churn_table[trainIndex, ]
testData <- churn_table[-trainIndex, ]

churn_table$random<-sample(0:1,size = nrow(churn_table),replace = T,prob = c(0.3,0.7)) 

rl<- glm(Churn ~ total_day_minutes + total_day_calls + number_customer_service_calls, trainData, family=binomial(link='logit'))

summary(rl)
```
'Total day minutes' and 'number customer service calls' are the most influential variables of the model (they contain ***), and we are able to know it too with Pr(>|z|) value. These two variables have very low p values, meaning a higher statistical significance.

To compare this model, another model will be developed with a different method: RandomForest. 

```{r construction of the classification model with randomforest}
model <- randomForest(Churn ~ total_day_minutes + total_day_calls + number_customer_service_calls, data = trainData)
print(model)
summary(model)
```

The confusion matrix is a tool that evaluates the performance of a classification model:

Confusion matrix:
      FALSE TRUE class.error
FALSE  2929   77  0.02561544
TRUE    332  163  0.67070707

Here we can see that 'False' represents when the client did not leave; 'True' shows when the client left the services, 2929 is the correct prediction of the clients who did not leave the company, 77 are the clients incorrectly predicted to leave, 332 are clients that left the services but were not predicted to do, and 163 are the clients correctly predicted to leave.

class.error: Classification error rate by class. For those who do not churn, the error rate is 2.63% (0.02628077), while for those who do churn, the error rate is much higher, at 67.47% (0.67474747). This indicates that the model has difficulty correctly predicting churn cases.

In summary, the model has a reasonably low error rate for predicting customers who do not churn, but it is not as effective at predicting customers who do churn, which is reflected in the high error rate for the TRUE class. Considering this, we are going to evaluate GLM model.


```{r confusion matrix}
confusion<-function(real,scoring,umbral){ 
  conf<-table(real,scoring>=umbral)
  if(ncol(conf)==2) return(conf) else return(NULL)
}

metrics<-function(matrix_conf){
  success <- (matrix_conf[1,1] + matrix_conf[2,2]) / sum(matrix_conf) *100
  precision <- matrix_conf[2,2] / (matrix_conf[2,2] + matrix_conf[1,2]) *100
  recall <- matrix_conf[2,2] / (matrix_conf[2,2] + matrix_conf[2,1]) *100
  F1 <- 2*precision*recall/(precision+recall)
  output<-c(success,precision,recall,F1)
  return(output)
}


thresholds<-function(real,scoring){
  thresholds<-data.frame(
threshold=rep(0,times=19),success=rep(0,times=19),precision=rep(0,times=19),recall=rep(0,times=19),F1=rep(0,times=19))
  cont <- 1
  for (cada in seq(0.05,0.95,by = 0.05)){
    data<-metrics(confusion(real,scoring,cada))
    register<-c(cada,data)
    thresholds[cont,]<-register
    cont <- cont + 1
  }
  return(thresholds)
}

```

```{r ROC and AUC}

roc<-function(prediction){
  r<-performance(prediction,'tpr','fpr')
  plot(r)
}

auc<-function(prediction){
  a<-performance(prediction,'auc')
  return(a@y.values[[1]])
}

```

```{r predictions glm}
rl_predict<-predict(rl,testData,type = 'response')
head(rl_predict)
```

```{r thresholds}
thr_rl<-thresholds(testData$Churn,rl_predict)
thr_rl
```
This is the summary of the metrics used to evaluate the model.

```{r thresholds F1}
thr_final_rl<-thr_rl[which.max(thr_rl$F1),1]
thr_final_rl
```

```{r conf matrix} 
confusion(testData$Churn,rl_predict,thr_final_rl)
```
This is the final confusion matrix obtained with GLM model. 

```{r metrics}
rl_metrics<-filter(thr_rl,threshold==thr_final_rl)
rl_metrics
```

```{r ROC curve}
#Creation of prediction object
rl_prediction<-prediction(rl_predict,testData$Churn)
#ROC
roc_curve <-roc(rl_prediction)
```

```{r AUC}
auc<-function(rl_prediction){
  a<-performance(rl_prediction,'auc')
  return(a@y.values[[1]])
}
```


```{r definitive metrics}
rl_metrics<-cbind(rl_metrics,AUC=round(auc(rl_prediction),2)*100)
print(t(rl_metrics))
```
An AUC of 74.00% suggests that the model has a good ability to distinguish between positive and negative instances. The AUC measures the model’s ability to correctly rank positive instances higher than negative ones.

Overall, the model shows a good accuracy and AUC but has room for improvement in precision and F1 score. Adjusting the threshold or fine-tuning the model could help enhance these metrics.

##4. Expected Results and Next Steps

At the end of the analysis, the goal is to have a list of factors that are highly related to the churn rate. This will allow the company three key takeaways:

*· Identify pain points:* If it is found that a high number of calls to customer service is associated with churn, the company could improve its customer service to resolve issues more efficiently and reduce churn.

*· Personalize offers:* With the segmentation into "High" and "Low" usage, specific offers can be designed for each group, improving satisfaction and reducing the likelihood of churn.

*· Improve predictive models:* Using the data and analysis, models can be created to more accurately predict which customers are at higher risk of leaving, allowing the company to take preventive measures.

These insights can take us further; let's consider a few questions: How can we reduce the churn rate more effectively? How would you select these clients in order to maximize the success and benefit of these actions? We should focus on applying this analysis to a specific target audience.

```{r prevent churn}
churn_all$churn_prob <- predict(rl, churn_all, type = "response")
top_500_customers <- churn_all %>% arrange(desc(churn_prob)) %>% head(500)
head(top_500_customers)
```

##5. Conclusions

· Customer service calls have a significant impact on churn.

· High-usage customers are more likely to leave.

· The logistic regression model has acceptable accuracy, suggesting it is suitable for predicting churn.

· Retention strategies should focus on customers with high usage and frequent interactions with customer service.

· Marketing campaigns can be more effective if they target a cluster of customers identified as having a higher likelihood of churn.